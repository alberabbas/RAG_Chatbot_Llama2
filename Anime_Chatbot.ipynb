{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Library installations and imports"
      ],
      "metadata": {
        "id": "_m59O5JyRmOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers\n",
        "!pip install ctransformers\n",
        "!pip install gradio\n",
        "!pip install langchain_pinecone\n",
        "!pip install pinecone-client\n",
        "!pip install langchain_community\n",
        "!pip install langchain\n",
        "!pip install pinecone\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "59TYrPou3tHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "import pinecone\n",
        "from pinecone import Pinecone\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from pinecone import ServerlessSpec\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "from ctransformers import AutoModelForCausalLM, AutoConfig"
      ],
      "metadata": {
        "id": "Bs5_HZ435iB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing RAG based data in Pinecone"
      ],
      "metadata": {
        "id": "hio3sI4i7zi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#File import\n",
        "df = pd.read_csv('/content/drive/MyDrive/anime_data.csv')\n",
        "\n",
        "\n",
        "#Pre-processing\n",
        "# Convert 'Synopsis' and 'Genre' columns from list to paragraph\n",
        "def list_to_paragraph(value):\n",
        "    if isinstance(value, str):\n",
        "        value = value.strip(\"[]\").replace(\"', '\", \", \").replace(\"'\", \"\")\n",
        "    return value\n",
        "\n",
        "df['Synopsis'] = df['Synopsis'].apply(list_to_paragraph)\n",
        "df['Genre'] = df['Genre'].apply(list_to_paragraph)\n",
        "\n",
        "\n",
        "#Lowercase\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = df[column].str.lower()\n",
        "\n",
        "\n",
        "#drop duplicates and extra columns\n",
        "df.drop_duplicates(subset=['Synopsis'], inplace=True)\n",
        "df = df.drop(['Rank', 'Popularity', 'Members'], axis=1)\n",
        "\n",
        "\n",
        "#Fill null values with hyphen\n",
        "df.fillna('-', inplace=True)\n",
        "df.isnull().sum()\n",
        "\n",
        "\n",
        "#Making it family friendly\n",
        "df = df[df['Genre'].str.contains('hentai') == False]\n",
        "\n",
        "\n",
        "#Concatenating into a single column with headers\n",
        "df['combined'] = df.apply(lambda row: f\"Title: {row['Title']}. Type: {row['Type']}. Status: {row['Status']}. Score: {row['Score']}. Studios: {row['Studios']}. Source: {row['Source']}. Genre: {row['Genre']}. Rated: {row['Rated']}. Synopsis: {row['Synopsis']}.\", axis=1)\n",
        "\n",
        "\n",
        "#Chunking the data\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
        "df['chunks'] = df['combined'].apply(lambda x: text_splitter.split_text(x))\n",
        "chunks = [chunk for sublist in df['chunks'] for chunk in sublist]\n",
        "\n",
        "\n",
        "# Using mpnet embedding with ThreadPoolExecutor\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "def embed_chunk(chunk):\n",
        "    return model.encode(chunk, convert_to_tensor=True)\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    embeddings = list(executor.map(embed_chunk, chunks))\n",
        "\n",
        "\n",
        "# Move tensors to CPU and convert to numpy arrays\n",
        "embeddings = [embedding.cpu().numpy() for embedding in embeddings]\n",
        "\n",
        "\n",
        "#Initializing Pinecone\n",
        "pinecone_client = Pinecone(\n",
        "    api_key='YOUR_API_KEY_HERE'\n",
        ")\n",
        "index_name = 'anime-index'\n",
        "if index_name not in pinecone_client.list_indexes().names():\n",
        "    pinecone_client.create_index(index_name,\n",
        "                                 dimension=embeddings[0].shape[0],\n",
        "                                 metric=\"cosine\",\n",
        "                                 spec=ServerlessSpec(\n",
        "                                     cloud=\"aws\",\n",
        "                                     region=\"us-east-1\")\n",
        "    )\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "\n",
        "#Creating Vector and Metadata to store\n",
        "num_rows = len(df)\n",
        "vectors = []\n",
        "for i, embedding in enumerate(embeddings[:num_rows]):\n",
        "    row = df.iloc[i]\n",
        "    metadata = {\n",
        "        \"Title\": row['Title'],\n",
        "        \"Type\": row['Type'],\n",
        "        \"Status\": row['Status'],\n",
        "        \"Score\": row['Score'],\n",
        "        \"Studios\": row['Studios'],\n",
        "        \"Source\": row['Source'],\n",
        "        \"Genre\": row['Genre'],\n",
        "        \"Rated\": row['Rated'],\n",
        "        \"Synopsis\": row['Synopsis']\n",
        "    }\n",
        "    vectors.append((str(i), embedding, metadata))\n",
        "\n",
        "\n",
        "# Upsert embeddings with metadata in batches\n",
        "batch_upsert(index, vectors)"
      ],
      "metadata": {
        "id": "KkI69bbZBuZg",
        "execution": {
          "iopub.status.busy": "2024-07-29T10:28:48.557974Z",
          "iopub.status.idle": "2024-07-29T10:28:48.558459Z",
          "shell.execute_reply.started": "2024-07-29T10:28:48.558212Z",
          "shell.execute_reply": "2024-07-29T10:28:48.558232Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM GENERATION"
      ],
      "metadata": {
        "id": "StMDxJtk7IJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "def initialize_pinecone(api_key, index_name):\n",
        "    pinecone_client = Pinecone(\n",
        "        api_key=api_key\n",
        "    )\n",
        "    index = pinecone_client.Index(index_name)\n",
        "    return index\n",
        "\n",
        "\n",
        "# Load LLAMA Smart Query model\n",
        "def load_query_refinement_model(model_path, model_file,device):\n",
        "    config = AutoConfig.from_pretrained(model_path, context_length=4096,gpu_layers=50)\n",
        "    config.device = device\n",
        "    query_refinement_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        model_file=model_file,\n",
        "        model_type=\"llama\",\n",
        "        config=config,\n",
        "        hf=False\n",
        "    )\n",
        "    return query_refinement_model\n",
        "\n",
        "\n",
        "#Load LLAMA QA Model\n",
        "def load_models(model_path, model_file, embedding_model_name, device):\n",
        "    config = AutoConfig.from_pretrained(model_path, context_length=4096, gpu_layers=50)\n",
        "    config.device = device\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        model_file=model_file,\n",
        "        model_type=\"llama\",\n",
        "        config=config,\n",
        "        hf=False\n",
        "    )\n",
        "    embedding_model = SentenceTransformer(embedding_model_name).to(device)\n",
        "    return model, embedding_model\n",
        "\n",
        "\n",
        "# Smart Query with LLaMA\n",
        "def refine_query_with_llama(model, concatenated_query):\n",
        "    sys_prompt = \"\"\"\n",
        "    Correct the grammar of the following query.\n",
        "\n",
        "    Do not add any new information about the anime.\n",
        "\n",
        "    Provide only the corrected query as a single line.\n",
        "\n",
        "    Do not include starting text telling what you did.\n",
        "    \"\"\"\n",
        "    prompt = f\"<s>[INST] <<SYS>>\\n{sys_prompt}\\n<</SYS>>\\n{concatenated_query} [/INST]\"\n",
        "\n",
        "    inputs = model(prompt, stream=True)\n",
        "    refined_query = \"\"\n",
        "    for response_part in inputs:\n",
        "        refined_query += response_part\n",
        "\n",
        "    return refined_query.strip()\n",
        "\n",
        "\n",
        "# Embed query\n",
        "def embed_query(query, embedding_model):\n",
        "    return embedding_model.encode(query, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "\n",
        "\n",
        "# Search Pinecone\n",
        "def search_pinecone(index, embedded_query, top_k=3):\n",
        "    query_results = index.query(vector=embedded_query, top_k=top_k, include_metadata=True)\n",
        "    return query_results['matches']\n",
        "\n",
        "\n",
        "# Generate response with LLaMA\n",
        "def generate_response_with_llama(model, conversation_history):\n",
        "    sys_prompt = \"\"\"\n",
        "    You are an assistant that provides precise information (up to two lines) about anime based on user queries.\n",
        "    You will receive 3 different contexts. Choose the one closest to the user query without mentioning the contexts in your response.\n",
        "    \"\"\"\n",
        "    formatted_prompt = f\"<s>[INST] <<SYS>>\\n{sys_prompt}\\n<</SYS>>\\n\"\n",
        "    for i, (user_msg, assistant_msg) in enumerate(conversation_history):\n",
        "        if i < len(conversation_history) - 1:\n",
        "            formatted_prompt += f\"{user_msg} [/INST] {assistant_msg} </s>\\n<s>[INST] \"\n",
        "        else:\n",
        "            formatted_prompt += f\"{user_msg} [/INST] \"\n",
        "    formatted_prompt = formatted_prompt.strip()\n",
        "\n",
        "    inputs = model(formatted_prompt, stream=True)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Handle user query and maintain history\n",
        "def handle_user_query(user_query, conversation_history, model, embedding_model, index,refinement_model, max_tokens=2000):\n",
        "    # Check if conversation_history has at least 3 entries\n",
        "    if len(conversation_history) >= 3:\n",
        "      last_entries = conversation_history[-3:]\n",
        "    else:\n",
        "      last_entries = conversation_history\n",
        "      #Add last response if it exists\n",
        "    try:\n",
        "      most_recent_entry_1 = conversation_history[-1][1]\n",
        "    except IndexError:\n",
        "      most_recent_entry_1 = \"\"\n",
        "# Concatenated query\n",
        "    concatenated_query = \" \".join([entry[0] for entry in last_entries] + ([most_recent_entry_1] if most_recent_entry_1 else []) + [user_query])\n",
        "    refined_query=refine_query_with_llama(refinement_model,concatenated_query)\n",
        "    embedded_query = embed_query(refined_query, embedding_model)\n",
        "    matches = search_pinecone(index, embedded_query)\n",
        "\n",
        "    context_info = \"\"\n",
        "    for match in matches:\n",
        "        context_info += f\"Title: {match['metadata']['Title']}. Type: {match['metadata']['Type']}. Status: {match['metadata']['Status']}. Score: {match['metadata']['Score']}. Studios: {match['metadata']['Studios']}. Source: {match['metadata']['Source']}. Genre: {match['metadata']['Genre']}. Rated: {match['metadata']['Rated']}. Synopsis: {match['metadata']['Synopsis']}.\\n\"\n",
        "\n",
        "    assistant_msg = f\"Context: {context_info}\\nAssistant:\"\n",
        "    conversation_history.append((f\"User: {user_query}\", assistant_msg))\n",
        "\n",
        "    total_tokens = sum(len(user_msg) + len(assistant_msg) for user_msg, assistant_msg in conversation_history) + 56\n",
        "    if total_tokens > max_tokens:\n",
        "        total_messages = len(conversation_history)\n",
        "        first_20_percent_idx = max(1, int(total_messages * 0.2))\n",
        "        last_50_percent_idx = max(1, int(total_messages * 0.5))\n",
        "        # Calculate the range to keep\n",
        "        keep_first_part = conversation_history[:first_20_percent_idx]\n",
        "        keep_last_part = conversation_history[-last_50_percent_idx:]\n",
        "        # Ensure the range does not overlap and covers complete messages\n",
        "        if keep_first_part[-1] == keep_last_part[0]:\n",
        "            keep_last_part = keep_last_part[1:]\n",
        "\n",
        "        conversation_history = keep_first_part + keep_last_part\n",
        "\n",
        "    return generate_response_with_llama(model, conversation_history)\n",
        "\n",
        "\n",
        "# Gradio interface function\n",
        "def gradio_interface(user_query, history):\n",
        "    # Update conversation history with the user's query\n",
        "    history.append((user_query, \"\"))\n",
        "    # Generate the response using the model\n",
        "    response_generator = handle_user_query(user_query, conversation_history, model, embedding_model, index, model1)\n",
        "    # Initialize an empty string to accumulate the response\n",
        "    accumulated_response = \"\"\n",
        "    # Yield the response parts one by one\n",
        "    for response_part in response_generator:\n",
        "        accumulated_response += response_part  # Append the new part to the accumulated response\n",
        "        # Update the last entry in the history with the current accumulated response\n",
        "        history[-1] = (history[-1][0], accumulated_response)\n",
        "        yield history, \"\"  # Yield the updated history\n",
        "    if conversation_history:\n",
        "        conversation_history[-1] = (conversation_history[-1][0], accumulated_response)\n",
        "    else:\n",
        "        conversation_history.append((f\"User: {user_query}\", accumulated_response))\n",
        "\n",
        "\n",
        "#New Chat function\n",
        "def clear_history():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    return []\n",
        "\n",
        "\n",
        "# Initialize\n",
        "API_KEY = 'YOUR_PINECONE_API_KEY'\n",
        "INDEX_NAME = 'anime-index'\n",
        "MODEL_PATH = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "MODEL_FILE = \"llama-2-7b-chat.Q5_K_M.gguf\"\n",
        "EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "index = initialize_pinecone(API_KEY, INDEX_NAME)\n",
        "model, embedding_model = load_models(MODEL_PATH, MODEL_FILE, EMBEDDING_MODEL_NAME, DEVICE)\n",
        "model1 = load_query_refinement_model(MODEL_PATH, MODEL_FILE,DEVICE)\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "# Gradio interface run\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        chatbot = gr.Chatbot(label=\"Chat History\")\n",
        "        user_input = gr.Textbox(label=\"User Query\")\n",
        "        clear_button = gr.Button(\"New chat\")\n",
        "\n",
        "        # Use the generator function directly\n",
        "        user_input.submit(gradio_interface, inputs=[user_input, chatbot], outputs=[chatbot, user_input])\n",
        "        clear_button.click(fn=clear_history, outputs=[chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=False)"
      ],
      "metadata": {
        "id": "x3ejQ95GmNj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}